# Method

說明主要的方法與架構，更多實驗細節紀錄在 `logs` 資料夾的文件

## Data Pre-Processing

說明資料前處理的方法，程式實作參考 `efficientnet.ipynb` 檔案

- 將類別欄位轉換為 One-Hot Encoding 格式，供模型訓練使用
- 調整影像大小，縮放到相同的解析度（456x456）
- 模型架構帶有正規化效果，直接輸入值域為 0 到 255 之間的向量

## Data Split

說明不同階段的資料拆分策略，對於資料的更多細部資訊可以參考 `data` 資料夾的文件

- 在 Training 階段使用將訓練資料以 80%、20% 的比例切分為訓練（train）與驗證（valid）資料集，用以驗證模型

## Deep Learning Method: EfficientNet

模型使用 EfficientNet 在 Keras 上基於 ImageNet 之預訓練模型作為特徵擷取器，模型的 Dropout 比例為 20%，預訓練模型的權重僅作為模型初始權重，訓練過程中允許調整，並依序接上 1 層 GlobalAveragePooling2D 及 1 層有 219 個節點的全連接層作為分類層，模型架構如下所示：

```console
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
efficientnetb7 (Functional)  (None, 15, 15, 2560)      64097687  
_________________________________________________________________
avg_pool (GlobalAveragePooli (None, 2560)              0         
_________________________________________________________________
pred (Dense)                 (None, 219)               560859    
=================================================================
Total params: 64,658,546
Trainable params: 64,347,819
Non-trainable params: 310,727
_________________________________________________________________
```

訓練過程中，採用 Categorical Cross Entropy 作為 Loss Function；此外，若 Loss 在持續 10 個 Epoch 內沒有下降，就將 Learning Rate 設為當前的 0.31 倍，若 Loss 在持續 50 個 Epoch 內沒有下降，就停止訓練；訓練結束後，會將訓練階段在驗證集擁有最佳表現之 Epoch 的權重作為模型的最終權重，模型的其餘參數包括：

- BatchSize 設為 128
- 優化器使用 Adam
- 學習率設為 5e-04
- Epoch 設為 500
- Earlystop 機制設為 50 個 Epoch

# Conclusion

在這個比賽，最終取得了 Public 前 11% (18/153) 及 Private 前 15% (23/153) 的成績

從 EfficientNet 在訓練與驗證階段的績效可以發現模型有 Overfitting 問題，過程中嘗試的幾種方法最終使測試集的 Weighted Precision 從 0.6637 提升至 0.7114，但距離完全解決 Overfitting 仍有不小距離，未來可以嘗試的改善方向有以下幾點：

嘗試使用其他資料增強方法，以及不同方法的搭配組合
嘗試使用正則化等模型訓練手段，降低擬合程度
嘗試使用較大的學習率，避免模型停在局部最佳解
使用 XGBoost 集成其他模型的預測結果可以顯著提升績效，在這個部分的未來改善方向則有以下幾點：

集成的時候不僅就現有模型進行權重調整，亦可以考慮移除部分績效較差的模型，這次在這個部分的實驗著墨較少
嘗試使用更大量的訓練資料，例如：總資料的 30%