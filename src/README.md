# Method

說明主要的方法與架構，更多實驗細節紀錄在 `logs` 資料夾的文件

## Data Pre-Processing

說明資料前處理的方法，程式實作參考 `efficientnet.ipynb` 檔案

- 將類別欄位轉換為 One-Hot Encoding 格式，供模型訓練使用
- 調整影像大小，縮放到相同的解析度（456x456）
- 模型架構帶有正規化效果，直接輸入值域為 0 到 255 之間的向量

## Data Split

說明不同階段的資料拆分策略，對於資料的更多細部資訊可以參考 `data` 資料夾的文件

- 在 Training 階段使用將訓練資料以 80%、20% 的比例切分為訓練（train）與驗證（valid）資料集，用以驗證模型

## Deep Learning Method: EfficientNet

模型使用 EfficientNet 在 Keras 上基於 ImageNet 之預訓練模型作為特徵擷取器，模型的 Dropout 比例為 20%，預訓練模型的權重僅作為模型初始權重，訓練過程中允許調整，並依序接上 1 層 GlobalAveragePooling2D 及 1 層有 219 個節點的全連接層作為分類層，模型架構如下所示：

```console
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
efficientnetb7 (Functional)  (None, 15, 15, 2560)      64097687  
_________________________________________________________________
avg_pool (GlobalAveragePooli (None, 2560)              0         
_________________________________________________________________
pred (Dense)                 (None, 219)               560859    
=================================================================
Total params: 64,658,546
Trainable params: 64,347,819
Non-trainable params: 310,727
_________________________________________________________________
```

訓練過程中，採用 Categorical Cross Entropy 作為 Loss Function；此外，若 Loss 在持續 10 個 Epoch 內沒有下降，就將 Learning Rate 設為當前的 0.31 倍，若 Loss 在持續 50 個 Epoch 內沒有下降，就停止訓練；訓練結束後，會將訓練階段在驗證集擁有最佳表現之 Epoch 的權重作為模型的最終權重，模型的其餘參數包括：

- BatchSize 設為 128
- 優化器使用 Adam
- 學習率設為 5e-04
- Epoch 設為 500
- Earlystop 機制設為 50 個 Epoch

# Conclusion

在這個比賽，最終取得了 Public 前 12% (35/275) 及 Private 前 13% (37/275) 的成績，績效分別為 0.873971 及 0.737483

根據實驗結果，可以得到以下結論：

- 使用遷移學習的時候，允許調整預訓練模型的權重可以使模型更適應資料集
- 使用較深的 EfficientNet 模型可以提升績效，但是在 B5 到 B7 之間的提升不明顯
- 由於每種蘭花僅有 10 張樣本，所以在訓練階段使用資料增強方法可以顯著提升績效達 6%
- 使用學習率遞減策略可以有效提升績效達 5%
